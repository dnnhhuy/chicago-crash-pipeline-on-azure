{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878e96f2-e67e-437a-9bf6-c5ec5847bfa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from delta import *\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "import itertools\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import Window\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77dfb60-5c3c-420b-8952-f1af7a774593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e593a65-9870-470e-b85c-2a49589b0f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "appId = ''\n",
    "clientSecret = ''\n",
    "tenantId = ''\n",
    "container_name = ''\n",
    "storage_account_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e96726e-7196-4cfd-b826-57c30cc58238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "       \"fs.azure.account.oauth2.client.id\": appId,\n",
    "       \"fs.azure.account.oauth2.client.secret\": clientSecret,\n",
    "       \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenantId}/oauth2/token\",\n",
    "       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n",
    "\n",
    "if not path_exists('mnt/chicago-crash'):\n",
    "       dbutils.fs.mount(\n",
    "       source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\",\n",
    "       mount_point = \"/mnt/chicago-crash\",\n",
    "       extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f249a6-6200-4e0f-a763-c540b2988d1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', \"io.delta:delta-iceberg_2.12:2.3.0.0\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('TransformData') \\\n",
    "    .config(conf=conf)\n",
    "    \n",
    "spark = configure_spark_with_delta_pip(spark).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "279be3b0-d9cc-4437-9066-f51efcc371f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_crash_table(days_ago=0):\n",
    "    crash_table = DeltaTable.forPath(spark, '/mnt/chicago-crash/raw/crashes')\n",
    "    crash_df = crash_table.toDF() \\\n",
    "        .filter((func.to_date(func.col('crash_date')) <= date.today()) & (func.to_date(func.col('crash_date')) >= date.today() - timedelta(days=days_ago))) \\\n",
    "        .withColumn('hour', func.hour(func.col('crash_date'))) \\\n",
    "        .withColumn('minute', func.minute(func.col('crash_date'))) \\\n",
    "        .withColumn('second', func.second(func.col('crash_date'))) \\\n",
    "        .withColumn('day', func.dayofmonth(func.col('crash_date'))) \\\n",
    "        .withColumn('dayofweek', func.dayofweek(func.col('crash_date'))) \\\n",
    "        .withColumn('month', func.month(func.col('crash_date'))) \\\n",
    "        .withColumn('week', func.weekofyear(func.col('crash_date'))) \\\n",
    "        .withColumn('year', func.year(func.col('crash_date'))) \\\n",
    "        .withColumn('quarter', func.quarter(func.col('crash_date'))) \\\n",
    "        .drop('location')\n",
    "    return crash_df\n",
    "\n",
    "def extract_people_table(days_ago=0):\n",
    "    people_table = DeltaTable.forPath(spark, '/mnt/chicago-crash/raw/people')\n",
    "    people_df = people_table.toDF() \\\n",
    "        .filter((func.to_date(func.col('crash_date')) <= date.today()) & (func.to_date(func.col('crash_date')) >= date.today() - timedelta(days=days_ago)))\n",
    "    return people_df\n",
    "\n",
    "def extract_vehicle_table(days_ago=0):\n",
    "    vehicles_table = DeltaTable.forPath(spark, '/mnt/chicago-crash/raw/vehicles')\n",
    "    vehicles_df = vehicles_table.toDF() \\\n",
    "        .filter((func.to_date(func.col('crash_date')) <= date.today()) & (func.to_date(func.col('crash_date')) >= date.today() - timedelta(days=days_ago))) \\\n",
    "        .filter(func.col('vehicle_id') != 'N/A')\n",
    "    return vehicles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2aa101d-a92b-4218-99a5-21c195240d51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform(crash_df, people_df, vehicle_df):\n",
    "\n",
    "    dim_vehicle = vehicle_df.select('vehicle_id', 'num_passengers', 'make', 'model', 'lic_plate_state', 'vehicle_year', 'vehicle_defect', 'vehicle_type', 'vehicle_use', 'travel_direction', 'maneuver', 'towed_i', 'fire_i', 'occupant_cnt', 'towed_by', 'towed_to', 'first_contact_point', 'commercial_src', 'carrier_name', 'carrier_state', 'carrier_city', 'total_vehicle_length', 'axle_cnt', 'vehicle_config', 'cargo_body_type', 'load_type')\n",
    "    \n",
    "    dim_person = people_df.select('person_id', 'person_type', 'seat_no', 'city', 'state', 'zipcode', 'sex', 'age', 'drivers_license_state', 'drivers_license_class', 'safety_equipment', 'airbag_deployed', 'ejection', 'injury_classification', 'hospital', 'driver_action', 'driver_vision', 'physical_condition', 'pedpedal_action', 'pedpedal_visibility', 'pedpedal_location', 'bac_result')\n",
    "\n",
    "    dim_location = crash_df.select('street_no', 'street_direction', 'street_name', 'alignment', 'posted_speed_limit', 'trafficway_type', 'longitude', 'latitude') \\\n",
    "        .dropDuplicates()\n",
    "        \n",
    "    dim_location = dim_location.withColumn('location_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_location.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('location_id', 'street_no', 'street_direction', 'street_name', 'alignment', 'posted_speed_limit', 'trafficway_type', 'longitude', 'latitude')\n",
    "\n",
    "    time = [[\"{:02d}\".format(x) for x in range(24)], [\"{:02d}\".format(x) for x in range(60)], [\"{:02d}\".format(x) for x in range(60)]]\n",
    "    combination = itertools.product(*time)\n",
    "\n",
    "    dim_time = spark.createDataFrame(combination, ['hour', 'minute', 'second'])\n",
    "    dim_time = dim_time \\\n",
    "        .withColumn('time_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_time.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('time_id', 'hour', 'minute', 'second')\n",
    "\n",
    "    dim_date = crash_df.select('day', 'dayofweek', 'month', 'week', 'year', 'quarter')\n",
    "    dim_date = dim_date \\\n",
    "        .dropDuplicates() \\\n",
    "        .withColumn('date_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_date.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('date_id', 'day', 'dayofweek', 'month', 'week', 'year', 'quarter')\n",
    "\n",
    "\n",
    "    dim_weather = crash_df.select('weather_condition', 'lighting_condition') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates()\n",
    "    dim_weather = dim_weather \\\n",
    "        .withColumn('weather_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_weather.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('weather_id', 'weather_condition', 'lighting_condition')\n",
    "        \n",
    "\n",
    "    dim_junk = crash_df.select('intersection_related_i', 'hit_and_run_i', 'photos_taken_i', 'statements_taken_i', 'dooring_i', 'work_zone_i', 'workers_present_i') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates()\n",
    "    dim_junk = dim_junk \\\n",
    "        .withColumn('junk_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_junk.columns\n",
    "            )),\n",
    "            512\n",
    "        ))\n",
    "        \n",
    "    dim_cause = crash_df.select('prim_contributory_cause', 'sec_contributory_cause') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates()\n",
    "    dim_cause = dim_cause \\\n",
    "        .withColumn('cause_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_cause.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('cause_id', 'prim_contributory_cause', 'sec_contributory_cause')\n",
    "\n",
    "\n",
    "    dim_crash_type = crash_df.select('crash_type') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates()\n",
    "    dim_crash_type = dim_crash_type \\\n",
    "        .withColumn('crash_type_id', func.sha2(\n",
    "            func.concat(*(\n",
    "                func.col(col).cast(\"string\")\n",
    "                for col \n",
    "                in dim_crash_type.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('crash_type_id', 'crash_type')\n",
    "\n",
    "    dim_report_type = crash_df.select('report_type') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates()\n",
    "    dim_report_type = dim_report_type \\\n",
    "        .withColumn('report_type_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_report_type.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('report_type_id', 'report_type')\n",
    "\n",
    "    dim_collision = crash_df.select('first_crash_type') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates() \\\n",
    "        .withColumnRenamed('first_crash_type', 'collision_type')\n",
    "    dim_collision = dim_collision \\\n",
    "        .withColumn('collision_type_id', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_collision.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('collision_type_id', 'collision_type')\n",
    "\n",
    "    dim_road_cond = crash_df.select('roadway_surface_cond', 'road_defect') \\\n",
    "        .na.fill('empty')\\\n",
    "        .dropDuplicates()\n",
    "    dim_road_cond = dim_road_cond \\\n",
    "        .withColumn ('road_cond_key', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_road_cond.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('road_cond_key', 'roadway_surface_cond', 'road_defect')\n",
    "\n",
    "    dim_control_device_cond = crash_df.select('traffic_control_device', 'device_condition') \\\n",
    "        .na.fill('empty') \\\n",
    "        .dropDuplicates()\n",
    "    dim_control_device_cond = dim_control_device_cond \\\n",
    "        .withColumn('device_cond_key', func.sha2(\n",
    "        func.concat(*(\n",
    "            func.col(col).cast(\"string\")\n",
    "            for col \n",
    "            in dim_control_device_cond.columns\n",
    "            )),\n",
    "            512\n",
    "        )) \\\n",
    "        .select('device_cond_key', 'traffic_control_device', 'device_condition')\n",
    "\n",
    "\n",
    "    bridge_vehicle_group = vehicle_df.select('crash_record_id', 'vehicle_id') \\\n",
    "        .withColumnRenamed('crash_record_id', 'vehicle_group_key')\n",
    "\n",
    "    bridge_person_group = people_df.select('crash_record_id', 'person_id') \\\n",
    "        .withColumnRenamed('crash_record_id', 'person_group_key')\n",
    "\n",
    "\n",
    "    windowspec = Window.partitionBy(func.col('location_id')).orderBy(func.col('crash_date'))\n",
    "    road_cond_mini_dim = crash_df.join(dim_location, (crash_df['street_no'] == dim_location['street_no'])\n",
    "                            & (crash_df['street_direction'] == dim_location['street_direction'])\n",
    "                            & (crash_df['street_name'] == dim_location['street_name'])\n",
    "                            & (crash_df['alignment'] == dim_location['alignment'])\n",
    "                            & (crash_df['posted_speed_limit'] == dim_location['posted_speed_limit'])\n",
    "                            & (crash_df['trafficway_type'] == dim_location['trafficway_type'])\n",
    "                            & (crash_df['longitude'] == dim_location['longitude'])\n",
    "                            & (crash_df['latitude'] == dim_location['latitude']), 'inner') \\\n",
    "                        .join(dim_road_cond, (crash_df['roadway_surface_cond'] == dim_road_cond['roadway_surface_cond'])\n",
    "                                            & (crash_df['road_defect'] == dim_road_cond['road_defect']), 'inner') \\\n",
    "                        .dropDuplicates() \\\n",
    "                        .select('crash_date', 'location_id', 'road_cond_key') \\\n",
    "                        .withColumn('start_date', func.to_date(func.col('crash_date')).cast(StringType())) \\\n",
    "                        .withColumn('end_date', func.to_date(func.lead(func.col('crash_date'), 1).over(windowspec)).cast(StringType())) \\\n",
    "                        .sort(func.col('location_id'), func.col('crash_date')) \\\n",
    "                        .select('location_id', 'road_cond_key', 'start_date', 'end_date') \\\n",
    "                        .na.fill(\"\")\n",
    "    \n",
    "    control_device_cond_mini_dim = crash_df.join(dim_location, (crash_df['street_no'] == dim_location['street_no'])\n",
    "                            & (crash_df['street_direction'] == dim_location['street_direction'])\n",
    "                            & (crash_df['street_name'] == dim_location['street_name'])\n",
    "                            & (crash_df['alignment'] == dim_location['alignment'])\n",
    "                            & (crash_df['posted_speed_limit'] == dim_location['posted_speed_limit'])\n",
    "                            & (crash_df['trafficway_type'] == dim_location['trafficway_type'])\n",
    "                            & (crash_df['longitude'] == dim_location['longitude'])\n",
    "                            & (crash_df['latitude'] == dim_location['latitude']), 'inner') \\\n",
    "                        .join(dim_control_device_cond, (crash_df['traffic_control_device'] == dim_control_device_cond['traffic_control_device'])\n",
    "                                            & (crash_df['device_condition'] == dim_control_device_cond['device_condition']), 'inner') \\\n",
    "                        .dropDuplicates() \\\n",
    "                        .select('crash_date', 'location_id', 'device_cond_key') \\\n",
    "                        .withColumn('start_date', func.to_date(func.col('crash_date')).cast(StringType())) \\\n",
    "                        .withColumn('end_date', func.to_date(func.lead(func.col('crash_date'), 1).over(windowspec)).cast(StringType())) \\\n",
    "                        .sort(func.col('location_id'), func.col('crash_date')) \\\n",
    "                        .select('location_id', 'device_cond_key', 'start_date', 'end_date') \\\n",
    "                        .na.fill(\"\")\n",
    "    \n",
    "    fact_crash = crash_df.join(dim_location, (crash_df['street_no'] == dim_location['street_no'])\n",
    "                            & (crash_df['street_direction'] == dim_location['street_direction'])\n",
    "                            & (crash_df['street_name'] == dim_location['street_name'])\n",
    "                            & (crash_df['alignment'] == dim_location['alignment'])\n",
    "                            & (crash_df['posted_speed_limit'] == dim_location['posted_speed_limit'])\n",
    "                            & (crash_df['trafficway_type'] == dim_location['trafficway_type'])\n",
    "                            & (crash_df['longitude'] == dim_location['longitude'])\n",
    "                            & (crash_df['latitude'] == dim_location['latitude']), 'inner') \\\n",
    "                        .join(dim_time, (crash_df['hour'] == dim_time['hour'])\n",
    "                                        & (crash_df['minute'] == dim_time['minute'])\n",
    "                                        & (crash_df['second'] == dim_time['second']), 'inner') \\\n",
    "                        .join(dim_date, (crash_df['day'] == dim_date['day'])\n",
    "                                        & (crash_df['dayofweek'] == dim_date['dayofweek'])\n",
    "                                        & (crash_df['month'] == dim_date['month'])\n",
    "                                        & (crash_df['week'] == dim_date['week'])\n",
    "                                        & (crash_df['year'] == dim_date['year']), 'inner') \\\n",
    "                        .join(dim_collision, (crash_df['first_crash_type'] == dim_collision['collision_type']), 'inner') \\\n",
    "                        .join(dim_report_type, (crash_df['report_type'] == dim_report_type['report_type']), 'inner') \\\n",
    "                        .join(dim_weather, (crash_df['weather_condition'] == dim_weather['weather_condition']) & (crash_df['lighting_condition'] == dim_weather['lighting_condition']), 'inner') \\\n",
    "                        .join(dim_junk, (crash_df['intersection_related_i'] == dim_junk['intersection_related_i'])\n",
    "                                        & (crash_df['hit_and_run_i'] == dim_junk['hit_and_run_i'])\n",
    "                                        & (crash_df['photos_taken_i'] == dim_junk['photos_taken_i'])\n",
    "                                        & (crash_df['statements_taken_i'] == dim_junk['statements_taken_i'])\n",
    "                                        & (crash_df['dooring_i'] == dim_junk['dooring_i'])\n",
    "                                        & (crash_df['work_zone_i'] == dim_junk['work_zone_i'])\n",
    "                                        & (crash_df['workers_present_i'] == dim_junk['workers_present_i']), 'inner') \\\n",
    "                        .join(dim_cause, (crash_df['prim_contributory_cause'] == dim_cause['prim_contributory_cause'])\n",
    "                                        & (crash_df['sec_contributory_cause'] ==  dim_cause['sec_contributory_cause']), 'inner') \\\n",
    "                        .join(dim_crash_type, (crash_df['crash_type'] == dim_crash_type['crash_type']), 'inner') \\\n",
    "                        .withColumn('person_group_key', func.col('crash_record_id')) \\\n",
    "                        .withColumn('vehicle_group_key', func.col('crash_record_id')) \\\n",
    "                        .select('location_id', 'time_id', 'date_id', 'person_group_key', 'vehicle_group_key', 'weather_id', 'junk_id', 'cause_id', 'collision_type_id', 'report_type_id', 'crash_type_id', 'damage', 'num_units', 'injuries_total', 'injuries_fatal', 'injuries_incapacitating', 'injuries_non_incapacitating', 'injuries_reported_not_evident', 'injuries_no_indication', 'injuries_unknown')\n",
    "    return {'dim_location': dim_location,\n",
    "            'road_cond_mini_dim': road_cond_mini_dim,\n",
    "            'control_device_cond_mini_dim': control_device_cond_mini_dim,\n",
    "            'dim_road_cond': dim_road_cond,\n",
    "            'dim_control_device_cond': dim_control_device_cond,\n",
    "            'dim_time': dim_time,\n",
    "            'dim_date': dim_date,\n",
    "            'bridge_vehicle_group': bridge_vehicle_group,\n",
    "            'dim_vehicle': dim_vehicle,\n",
    "            'dim_collision': dim_collision,\n",
    "            'dim_report_type': dim_report_type,\n",
    "            'bridge_person_group': bridge_person_group,\n",
    "            'dim_person': dim_person,\n",
    "            'dim_weather': dim_weather,\n",
    "            'dim_junk': dim_junk,\n",
    "            'dim_cause': dim_cause,\n",
    "            'dim_crash_type': dim_crash_type,\n",
    "            'fact_crash': fact_crash}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfbcd19-a940-4ab4-a7aa-de77c015650e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge to old table\n",
    "def load_star_schema(table_dict):\n",
    "    for table_name, table in table_dict.items():\n",
    "        if not path_exists(f'/mnt/chicago-crash/serving/{table_name}'):\n",
    "            table.write.mode('overwrite').format('delta').option('path', f'/mnt/chicago-crash/serving/{table_name}').option('mergeSchema', True).save()\n",
    "        else:\n",
    "            old_table = DeltaTable.forPath(spark, path=f'/mnt/chicago-crash/serving/{table_name}')\n",
    "            if table_name == 'dim_location':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.location_id = b.location_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_road_cond':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.road_cond_key = b.road_cond_key') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_control_device_cond':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.device_cond_key = b.device_cond_key') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'road_cond_mini_dim':\n",
    "                old_table_df = old_table.toDF()\n",
    "                old_table_df.createOrReplaceTempView('OldTableTempView')\n",
    "                # Get max start date from old table\n",
    "                max_StartDate = spark.sql('SELECT location_id, max(start_date) as max_start_date FROM OldTableTempView GROUP BY location_id')\n",
    "\n",
    "                # Filter only new records having start_date > max start date from old table\n",
    "                new_record = table.join(max_StartDate, (table['location_id'] == max_StartDate['location_id']), 'left_outer') \\\n",
    "                    .filter((func.col('start_date') > func.col('max_start_date')) | (func.isnull(func.col('max_start_date')))) \\\n",
    "                    .select(table['location_id'], 'road_cond_key', 'start_date', 'end_date')\n",
    "                \n",
    "                new_record.createOrReplaceTempView('NewTableTempView')\n",
    "                \n",
    "                # Get min start date from new records\n",
    "                min_StartDate_newRecord = spark.sql('SELECT location_id, min(start_date) as min_start_date FROM NewTableTempView GROUP BY location_id')\n",
    "\n",
    "                # Filter neededly updated rows from old table using min start date from above\n",
    "                update_rows = old_table_df.join(min_StartDate_newRecord, (old_table_df['location_id'] == min_StartDate_newRecord['location_id']), 'inner') \\\n",
    "                    .filter(func.col('end_date') == '') \\\n",
    "                    .select(old_table_df['location_id'], 'road_cond_key', 'start_date', 'min_start_date') \\\n",
    "                    .withColumnRenamed('min_start_date', 'end_date')\n",
    "                \n",
    "                insert_rows = update_rows.union(new_record)\n",
    "                 \n",
    "                old_table.alias('a') \\\n",
    "                    .merge(insert_rows.alias('b'), condition='a.location_id = b.location_id AND a.road_cond_key = b.road_cond_key AND a.start_date = b.start_date') \\\n",
    "                    .whenMatchedUpdate(set={'end_date': 'b.end_date'}) \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "                    \n",
    "            elif table_name == 'control_device_cond_mini_dim':\n",
    "                old_table_df = old_table.toDF()\n",
    "                old_table_df.createOrReplaceTempView('OldTableTempView')\n",
    "                # Get max start date from old table\n",
    "                max_StartDate = spark.sql('SELECT location_id, max(start_date) as max_start_date FROM OldTableTempView GROUP BY location_id')\n",
    "\n",
    "                # Filter only new records\n",
    "                new_record = table.join(max_StartDate, (table['location_id'] == max_StartDate['location_id']), 'left_outer') \\\n",
    "                    .filter((func.col('start_date') > func.col('max_start_date')) | (func.isnull(func.col('max_start_date')))) \\\n",
    "                    .select(table['location_id'], 'device_cond_key', 'start_date', 'end_date')\n",
    "                \n",
    "                new_record.createOrReplaceTempView('NewTableTempView')\n",
    "                \n",
    "                # Get min start date from new records\n",
    "                min_StartDate_newRecord = spark.sql('SELECT location_id, min(start_date) as min_start_date FROM NewTableTempView GROUP BY location_id')\n",
    "\n",
    "                # Filter neededly updated rows from old table using min start date from above\n",
    "                update_rows = old_table_df.join(min_StartDate_newRecord, (old_table_df['location_id'] == min_StartDate_newRecord['location_id']), 'inner') \\\n",
    "                    .filter(func.col('end_date') == '') \\\n",
    "                    .select(old_table_df['location_id'], 'device_cond_key', 'start_date', 'min_start_date') \\\n",
    "                    .withColumnRenamed('min_start_date', 'end_date')\n",
    "                \n",
    "                insert_rows = update_rows.union(new_record)\n",
    "                 \n",
    "                old_table.alias('a') \\\n",
    "                    .merge(insert_rows.alias('b'), condition='a.location_id = b.location_id AND a.device_cond_key = b.device_cond_key AND a.start_date = b.start_date') \\\n",
    "                    .whenMatchedUpdate(set={'end_date': 'b.end_date'}) \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "                    \n",
    "            elif table_name == 'dim_time':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.time_id = b.time_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_date':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.date_id = b.date_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'bridge_vehicle_group':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.vehicle_group_key = b.vehicle_group_key AND a.vehicle_id = b.vehicle_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_vehicle':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.vehicle_id = b.vehicle_id') \\\n",
    "                    .whenMatchedUpdateAll() \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_collision':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.collision_type_id = b.collision_type_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_report_type':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.report_type_id = b.report_type_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'bridge_person_group':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.person_group_key = b.person_group_key and a.person_id = b.person_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_person':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.person_id = b.person_id') \\\n",
    "                    .whenMatchedUpdateAll() \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_weather':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.weather_id = b.weather_id') \\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_junk':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.junk_id = b.junk_id') \\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_cause':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.cause_id = b.cause_id') \\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .execute()\n",
    "            elif table_name == 'dim_crash_type':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.crash_type_id = b.crash_type_id')\\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .execute()\n",
    "            elif table_name == 'fact_crash':\n",
    "                old_table.alias('a') \\\n",
    "                    .merge(table.alias('b'), condition='a.location_id = b.location_id AND a.time_id = b.time_id AND a.date_id = b.date_id AND a.person_group_key = b.person_group_key AND a.vehicle_group_key = b.vehicle_group_key AND a.weather_id = b.weather_id AND a.junk_id = b.junk_id AND a.cause_id = b.cause_id AND a.collision_type_id = b.collision_type_id AND a.report_type_id = b.report_type_id AND a.crash_type_id = b.crash_type_id') \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091efeac-7a79-49c8-9a04-d2e75d8d53c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crash_df = extract_crash_table(days_ago=365)\n",
    "people = extract_people_table(days_ago=365)\n",
    "vehicles = extract_vehicle_table(days_ago=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1094333-c27b-44f3-b04f-ed47ff64a2ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_dict = transform(crash_df, people, vehicles)\n",
    "load_star_schema(table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23173f0d-58ca-4b06-8bdc-58da2c83dc55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if path_exists('/mnt/chicago-crash'):\n",
    "    dbutils.fs.unmount('/mnt/chicago-crash')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
